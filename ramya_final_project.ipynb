# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('hospital_readmissions.csv')

# 1. Dataset Overview
print("Dataset Shape:", df.shape)
print("\nDataset Info:")
print(df.info())
print("\nMissing Values:")
print(df.isnull().sum())

# Set Seaborn style
sns.set_style('whitegrid')

# 2. Class Distribution (readmitted)
plt.figure(figsize=(8, 6))
sns.countplot(x='readmitted', data=df)
plt.title('Distribution of Readmission (Target Variable)')
plt.xlabel('Readmitted')
plt.ylabel('Count')


# 3. Numerical Features Distribution
numerical_cols = ['time_in_hospital', 'n_lab_procedures', 'n_procedures',
                 'n_medications', 'n_outpatient', 'n_inpatient', 'n_emergency']
fig, axes = plt.subplots(4, 2, figsize=(12, 16))
axes = axes.flatten()
for i, col in enumerate(numerical_cols):
    sns.histplot(df[col], bins=30, ax=axes[i])
    axes[i].set_title(f'Distribution of {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Count')
plt.tight_layout()
plt.savefig('numerical_distributions.png')


# 4. Categorical Features Distribution (select key features)
categorical_cols = ['age', 'medical_specialty', 'diag_1', 'glucose_test', 'A1Ctest']
fig, axes = plt.subplots(3, 2, figsize=(12, 12))
axes = axes.flatten()
for i, col in enumerate(categorical_cols):
    sns.countplot(y=col, data=df, ax=axes[i], order=df[col].value_counts().index[:10])
    axes[i].set_title(f'Distribution of {col}')
    axes[i].set_xlabel('Count')
    axes[i].set_ylabel(col)
axes[-1].axis('off')  # Hide extra subplot
plt.tight_layout()
plt.savefig('categorical_distributions.png')

# 5. Correlation Matrix (numerical features)
plt.figure(figsize=(10, 8))
sns.heatmap(df[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Numerical Features')
plt.savefig('correlation_matrix.png')

# 6. Relationship between key features and readmission
plt.figure(figsize=(10, 6))
sns.boxplot(x='readmitted', y='time_in_hospital', data=df)
plt.title('Time in Hospital vs. Readmission')
plt.xlabel('Readmitted')
plt.ylabel('Time in Hospital (days)')
plt.savefig('time_in_hospital_vs_readmitted.png')

# Check for missing values
print("Missing Values:")
print(df.isnull().sum())

# Drop any unexpected NaNs
df = df.dropna()

from sklearn.preprocessing import LabelEncoder, StandardScaler

# Encode categorical variables
categorical_cols = ['age', 'medical_specialty', 'diag_1', 'diag_2', 'diag_3',
                    'glucose_test', 'A1Ctest', 'change', 'diabetes_med', 'readmitted']
le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col].astype(str))

# Standardize numerical features
numerical_cols = ['time_in_hospital', 'n_lab_procedures', 'n_procedures',
                  'n_medications', 'n_outpatient', 'n_inpatient', 'n_emergency']
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Detect outliers using IQR
def detect_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]
    return len(outliers)

# Print outliers for numerical columns
print("\nOutliers in Numerical Columns:")
for col in numerical_cols:
    print(f"{col}: {detect_outliers(df, col)} outliers")

# Save cleaned dataset
df.to_csv('cleaned_hospital_readmissions.csv', index=False)
print("\nCleaned dataset saved as 'cleaned_hospital_readmissions.csv'")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier

# Load the dataset
df = pd.read_csv('cleaned_hospital_readmissions.csv')

# Create new features
df['n_total_visits'] = df['n_outpatient'] + df['n_inpatient'] + df['n_emergency']
df['time_med_interaction'] = df['time_in_hospital'] * df['n_medications']
df['lab_proc_per_day'] = df['n_lab_procedures'] / (df['time_in_hospital'] + 1)  # Avoid division by zero

# Feature importance using Random Forest
X = df.drop('readmitted', axis=1)
y = df['readmitted']
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# Plot feature importance
feature_importance = pd.Series(rf.feature_importances_, index=X.columns)
plt.figure(figsize=(12, 6))
sns.barplot(x=feature_importance.values, y=feature_importance.index)
plt.title('Feature Importance from Random Forest')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.savefig('feature_importance.png')

# Select top 10 features
top_features = feature_importance.sort_values(ascending=False).head(10).index
print("Top 10 Features:", top_features.tolist())

X = df[top_features]
y = df['readmitted']


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)


from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))


import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance[top_features], y=top_features)
plt.title('Top 10 Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.tight_layout()
plt.show()


from sklearn.model_selection import train_test_split

X = df[top_features]
y = df['readmitted']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the model
logreg = LogisticRegression(max_iter=1000, random_state=42)
logreg.fit(X_train, y_train)

# Predict
y_pred_log = logreg.predict(X_test)

# Evaluation
print("ðŸ”¹ Logistic Regression Results ðŸ”¹")
print("Accuracy:", accuracy_score(y_test, y_pred_log))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_log))
print("\nClassification Report:\n", classification_report(y_test, y_pred_log))


from sklearn.neural_network import MLPClassifier

# Initialize and train the model
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)
mlp.fit(X_train, y_train)

# Predict
y_pred_mlp = mlp.predict(X_test)

# Evaluation
print("ðŸ”¹ Neural Network (MLPClassifier) Results ðŸ”¹")
print("Accuracy:", accuracy_score(y_test, y_pred_mlp))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_mlp))
print("\nClassification Report:\n", classification_report(y_test, y_pred_mlp))

import numpy as np
from sklearn.model_selection import learning_curve

def plot_learning_curve(estimator, title, X, y, cv=5):
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv,
                                                             scoring='accuracy',
                                                             train_sizes=np.linspace(0.1, 1.0, 5),
                                                             random_state=42)
    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)

    plt.figure(figsize=(8, 5))
    plt.plot(train_sizes, train_scores_mean, 'o-', label="Training Accuracy")
    plt.plot(train_sizes, test_scores_mean, 'o-', label="Validation Accuracy")
    plt.title(title)
    plt.xlabel("Training Set Size")
    plt.ylabel("Accuracy")
    plt.legend(loc="best")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Call for each model
plot_learning_curve(rf, "Learning Curve - Random Forest", X_train, y_train)
plot_learning_curve(logreg, "Learning Curve - Logistic Regression", X_train, y_train)
plot_learning_curve(mlp, "Learning Curve - Neural Network (MLP)", X_train, y_train)


# Align the columns of X_test with X_train
X_test = X_test[X_train.columns]


print("Training Features:", X_train.columns)
print("Test Features:", X_test.columns)


# Re-train the models (just in case the issue happened during the initial training)
rf.fit(X_train, y_train)
logreg.fit(X_train, y_train)
mlp.fit(X_train, y_train)


# Collect accuracies for each model
acc_rf = accuracy_score(y_test, rf.predict(X_test))
acc_log = accuracy_score(y_test, logreg.predict(X_test))
acc_mlp = accuracy_score(y_test, mlp.predict(X_test))

# Create a comparison dataframe
comparison_df = pd.DataFrame({
    'Model': ['Random Forest', 'Logistic Regression', 'Neural Network (MLP)'],
    'Accuracy': [acc_rf, acc_log, acc_mlp]
})

# Sort by accuracy
comparison_df = comparison_df.sort_values(by='Accuracy', ascending=False)

# Plot the comparison graph
plt.figure(figsize=(8, 6))
plt.bar(comparison_df['Model'], comparison_df['Accuracy'], color=['blue', 'green', 'red'])
plt.title('Model Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim([0, 1])  # Accuracy ranges from 0 to 1
plt.show()
